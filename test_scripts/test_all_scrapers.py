#!/usr/bin/env python3
"""
Test All Active Scrapers: LinkedIn, Indeed, Naukri, GitHub, StackOverflow
"""

import asyncio
import sys
sys.path.insert(0, '.')

from src.mcp_tools import ScraperMCPTools
from src.vector_db import CandidateVectorDB
from src.models import Candidate
from src.config import Config

async def main():
    print("="*80)
    print("TESTING ALL ACTIVE SCRAPERS")
    print("="*80)
    
    # Initialize
    print("\nğŸ“¦ Initializing components...")
    config = Config.load_yaml_config()
    
    # Initialize persistent browsers
    print("ğŸŒ Initializing persistent browsers...")
    try:
        from naukri_persistent_browser import naukri_browser_manager, linkedin_browser_manager
        print("  âœ… Naukri browser manager ready")
        print("  âœ… LinkedIn browser manager ready")
    except Exception as e:
        print(f"  âš ï¸  Could not initialize browser managers: {e}")
    
    mcp_tools = ScraperMCPTools(config)
    vector_db = CandidateVectorDB()
    
    print(f"âœ… MCP Tools initialized with {len(mcp_tools.scrapers)} scrapers")
    print(f"âœ… Vector DB initialized")
    
    # Show available scrapers
    print("\nğŸ”§ Available Scrapers:")
    for name in mcp_tools.scrapers.keys():
        print(f"  - {name}")
    
    # Test parameters
    job_title = "Python Developer"
    location = "Jaipur"
    skills = ["python", "django"]
    
    print(f"\nğŸ” Search Parameters:")
    print(f"  Job Title: {job_title}")
    print(f"  Location: {location}")
    print(f"  Skills: {', '.join(skills)}")
    
    # Test each scraper individually
    print("\n" + "="*80)
    print("TESTING INDIVIDUAL SCRAPERS")
    print("="*80)
    
    results = {}
    
    # 1. Test LinkedIn
    print("\n1ï¸âƒ£  Testing LinkedIn...")
    try:
        result = await mcp_tools.scrape_linkedin(
            job_title=job_title,
            location=location,
            skills=skills
        )
        results['linkedin'] = result['count']
        print(f"   âœ… LinkedIn: Found {result['count']} candidates")
    except Exception as e:
        print(f"   âŒ LinkedIn error: {e}")
        results['linkedin'] = 0
    
    # 2. Test Indeed
    print("\n2ï¸âƒ£  Testing Indeed...")
    try:
        result = await mcp_tools.scrape_indeed(
            job_title=job_title,
            location=location,
            skills=skills
        )
        results['indeed'] = result['count']
        print(f"   âœ… Indeed: Found {result['count']} candidates")
    except Exception as e:
        print(f"   âŒ Indeed error: {e}")
        results['indeed'] = 0
    
    # 3. Test Naukri
    print("\n3ï¸âƒ£  Testing Naukri...")
    try:
        result = await mcp_tools.scrape_naukri(
            job_title=job_title,
            location=location,
            skills=skills
        )
        results['naukri'] = result['count']
        print(f"   âœ… Naukri: Found {result['count']} candidates")
    except Exception as e:
        print(f"   âŒ Naukri error: {e}")
        results['naukri'] = 0
    
    # 4. Test GitHub
    print("\n4ï¸âƒ£  Testing GitHub...")
    try:
        result = await mcp_tools.scrape_github(skills=skills)
        results['github'] = result['count']
        print(f"   âœ… GitHub: Found {result['count']} candidates")
    except Exception as e:
        print(f"   âŒ GitHub error: {e}")
        results['github'] = 0
    
    # 5. Test StackOverflow
    print("\n5ï¸âƒ£  Testing StackOverflow...")
    try:
        result = await mcp_tools.scrape_github(skills=skills)  # Using same method
        results['stackoverflow'] = result['count']
        print(f"   âœ… StackOverflow: Found {result['count']} candidates")
    except Exception as e:
        print(f"   âŒ StackOverflow error: {e}")
        results['stackoverflow'] = 0
    
    # Test parallel scraping
    print("\n" + "="*80)
    print("TESTING PARALLEL SCRAPING")
    print("="*80)
    
    print("\nâš¡ Running all scrapers in parallel...")
    try:
        parallel_result = await mcp_tools.scrape_all_parallel(
            job_title=job_title,
            scrapers=['linkedin', 'indeed', 'naukri', 'github', 'stackoverflow'],
            location=location,
            skills=skills
        )
        
        print(f"\nâœ… Parallel scraping complete!")
        print(f"   Total unique candidates: {parallel_result['total_count']}")
        print(f"   Sources used: {', '.join(parallel_result['sources'])}")
        print(f"\n   Breakdown by source:")
        for source, count in parallel_result['by_source'].items():
            print(f"     - {source}: {count} candidates")
        
        # Store in Vector DB
        if parallel_result['total_count'] > 0:
            print(f"\nğŸ’¾ Storing {parallel_result['total_count']} candidates in Vector DB...")
            candidates = [Candidate(**c) for c in parallel_result['candidates']]
            added = vector_db.add_candidates(candidates)
            print(f"   âœ… Added {added} candidates to Vector DB")
            
            # Test similarity search
            print(f"\nğŸ” Testing similarity search...")
            similar = vector_db.search_similar(
                query="experienced Python developer with Django",
                n_results=5
            )
            print(f"   âœ… Found {len(similar)} similar candidates:")
            for i, candidate in enumerate(similar[:3], 1):
                meta = candidate['metadata']
                print(f"     {i}. {meta['name']} - {meta['title']}")
                print(f"        Source: {meta['source']} | Location: {meta['location']}")
    
    except Exception as e:
        print(f"   âŒ Parallel scraping error: {e}")
    
    # Summary
    print("\n" + "="*80)
    print("SUMMARY")
    print("="*80)
    
    print("\nğŸ“Š Results by Scraper:")
    total = 0
    for scraper, count in results.items():
        status = "âœ…" if count > 0 else "âŒ"
        print(f"   {status} {scraper.capitalize()}: {count} candidates")
        total += count
    
    print(f"\n   Total candidates found: {total}")
    
    # Vector DB stats
    stats = vector_db.get_stats()
    print(f"\nğŸ’¾ Vector DB Statistics:")
    print(f"   Total stored: {stats['total_candidates']} candidates")
    print(f"   By source: {stats['by_source']}")
    
    print("\n" + "="*80)
    print("âœ… TEST COMPLETE")
    print("="*80)
    print("\nActive Scrapers:")
    print("  âœ… LinkedIn (requires credentials)")
    print("  âœ… Indeed")
    print("  âœ… Naukri (Indian job portal)")
    print("  âœ… GitHub (developer profiles)")
    print("  âœ… StackOverflow (developer profiles)")
    print("\nNext: Run API server with: python3 run_api.py")
    print("="*80)

if __name__ == "__main__":
    asyncio.run(main())
